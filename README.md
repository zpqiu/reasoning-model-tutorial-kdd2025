Welcome to our KDD’25 Tutorial, “Reasoning Model Tutorial”.

Website: TBD


##  Tutorial Date, Time, Location
- **Date**: Sunday, 08/14/2022 (mm/dd/yyyy)
- **Time**: 9:00 am - 12:00 pm
- **Location**: Room 207B, Washington DC Convention Center, USA


## Tutorial Abstract

DeepSeek R1 is an open-source large language model dedicated to solving logical reasoning tasks. It employs a Mixture of Experts (MoE) architecture and boasts 671B parameters. Through reinforcement learning, it has been trained to perform deep thinking (generating long-chain-of-thought), excelling in reasoning tasks and various specialized fields such as mathematics, programming, and scientific analysis.

Moreover, as per the DeepSeek-R1 paper, the reasoning patterns of larger models can be distilled into smaller ones. Specifically, we can distill long-chain-of-thought (long-CoT) data that encapsulates reasoning processes from DeepSeek-R1 and directly fine-tune open-source models like Qwen and Llama. This simple distillation approach greatly enhances the reasoning capabilities of smaller models.

To illustrate the complete distillation process, we have prepared two notebooks demonstrating how to extract reasoning data from DeepSeek-R1 using the NIM API, and how to train models with the distilled data.

## Tutorial Materials and Outline

### Tutorial [[slides]](TBD)
### Tutorial Outline

1. Introduction
   
   
2. Preliminaries 
  
3. Distillation

4. Further Reading:
   - AI for Time Series (AI4TS) Papers, Tutorials, and Surveys [\[GitHub link\]](https://github.com/qingsongedu/awesome-AI-for-time-series-papers)



## Key References Published by Lecturers
TBD


## Short Bio of Lecturers

<img align="left" src="TBD" width="180" > **[Me](TBD)** this is me. 



## Cite our work
TBD

## Related Tutorial
- TBD
