<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>🧠 Incentivizing Reasoning in LLMs | KDD 2025 Tutorial</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');
        
        :root {
            --nvidia-green: #76b900;
            --nvidia-dark: #1a4d00;
            --toronto-blue: #003366;
            --accent-orange: #ff6f00;
            --modern-gray: #f8f9fa;
            --dark-text: #2c3e50;
            --light-text: #6c757d;
            --card-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
            --border-radius: 8px;
        }
        
        * {
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
            line-height: 1.6;
            color: var(--dark-text);
            background: var(--modern-gray);
            margin: 0;
            padding: 0;
        }
        
        .page-header {
            background: linear-gradient(135deg, var(--toronto-blue) 0%, var(--nvidia-dark) 70%, var(--nvidia-green) 100%);
            position: relative;
            overflow: hidden;
            padding: 4rem 0;
            text-align: center;
        }
        
        .page-header::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: url('assets/images/KDD25-Hero1-100.jpg') no-repeat center;
            background-size: cover;
            opacity: 0.3;
        }
        
        .project-name {
            font-size: clamp(2rem, 5vw, 3.5rem);
            font-weight: 700;
            color: white;
            text-shadow: 2px 2px 8px rgba(0, 0, 0, 0.4);
            margin-bottom: 1rem;
            position: relative;
            z-index: 2;
        }
        
        .project-tagline {
            font-size: clamp(1rem, 2.5vw, 1.25rem);
            color: rgba(255, 255, 255, 0.95);
            font-weight: 400;
            max-width: 900px;
            margin: 0 auto;
            line-height: 1.7;
            position: relative;
            z-index: 2;
        }
        
        .main-content {
            max-width: 1100px;
            margin: 0 auto;
            padding: 2rem 1rem;
            background: white;
            border-radius: var(--border-radius);
            box-shadow: var(--card-shadow);
            margin-top: -2rem;
            position: relative;
            z-index: 3;
        }
        
        .modern-card {
            background: white;
            border-radius: var(--border-radius);
            padding: 2rem;
            margin: 2rem 0;
            box-shadow: var(--card-shadow);
            border: 1px solid #e9ecef;
            transition: transform 0.2s ease, box-shadow 0.2s ease;
        }
        
        .modern-card:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 25px -5px rgba(0, 0, 0, 0.1);
        }
        
        .event-info {
            background: linear-gradient(135deg, var(--nvidia-green), var(--toronto-blue));
            color: white;
            padding: 2rem;
            border-radius: var(--border-radius);
            margin: 2rem 0;
            text-align: center;
        }
        
        .event-detail {
            font-size: 1.2rem;
            font-weight: 500;
            margin: 0.5rem 0;
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 0.5rem;
        }
        
        h1, h2 {
            color: var(--dark-text);
            font-weight: 600;
            margin-top: 3rem;
            margin-bottom: 1.5rem;
            position: relative;
        }
        
        h1::after, h2::after {
            content: '';
            position: absolute;
            bottom: -0.5rem;
            left: 0;
            width: 60px;
            height: 3px;
            background: linear-gradient(90deg, var(--nvidia-green), var(--accent-orange));
            border-radius: 2px;
        }
        
        h1 {
            font-size: 2.5rem;
            text-align: center;
        }
        
        h2 {
            font-size: 2rem;
        }
        
        .brand-badges {
            text-align: center;
            margin-bottom: 2rem;
        }
        
        .brand-badge {
            display: inline-block;
            padding: 0.5rem 1rem;
            border-radius: 25px;
            font-weight: 600;
            font-size: 1rem;
            margin: 0.5rem;
            color: white;
        }
        
        .nvidia-badge {
            background: var(--nvidia-green);
        }
        
        .kdd-badge {
            background: var(--toronto-blue);
        }
        
        a {
            color: var(--nvidia-green);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.2s ease;
        }
        
        a:hover {
            color: var(--nvidia-dark);
            text-decoration: underline;
        }
        
        .footer-info {
            text-align: center;
            padding: 2rem 0;
            background: var(--modern-gray);
            color: var(--light-text);
            font-size: 0.9rem;
        }
        
        .nvidia-text {
            color: var(--nvidia-green);
            font-weight: 600;
        }
        
        .kdd-text {
            color: var(--toronto-blue);
            font-weight: 600;
        }
        
        @media (max-width: 768px) {
            .page-header {
                padding: 2rem 0;
            }
            
            .main-content {
                padding: 1.5rem 1rem;
                margin-top: -1rem;
            }
            
            .event-detail {
                flex-direction: column;
                gap: 0.2rem;
            }
        }

        /* Additional styles for new content */
        .outline-grid {
            display: grid;
            gap: 1.5rem;
            margin: 2rem 0;
        }
        
        .outline-item {
            background: white;
            border-radius: var(--border-radius);
            padding: 1.5rem;
            box-shadow: var(--card-shadow);
            border-left: 4px solid var(--nvidia-green);
            transition: transform 0.2s ease, box-shadow 0.2s ease;
        }
        
        .outline-item:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 25px -5px rgba(0, 0, 0, 0.1);
        }
        
        .item-header {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            margin-bottom: 1rem;
            flex-wrap: wrap;
            gap: 0.5rem;
        }
        
        .item-title {
            font-size: 1.2rem;
            font-weight: 600;
            color: var(--dark-text);
            flex: 1;
            min-width: 200px;
        }
        
        .item-duration {
            background: var(--nvidia-green);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 15px;
            font-size: 0.85rem;
            font-weight: 500;
            white-space: nowrap;
        }
        
        .item-description {
            color: var(--light-text);
            line-height: 1.6;
        }
        
        .instructor-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2rem;
            margin: 2rem 0;
        }
        
        .instructor-card {
            background: white;
            border-radius: var(--border-radius);
            padding: 2rem;
            box-shadow: var(--card-shadow);
            border: 1px solid #e9ecef;
            transition: transform 0.2s ease, box-shadow 0.2s ease;
            text-align: center;
        }
        
        .instructor-card:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 25px -5px rgba(0, 0, 0, 0.1);
        }
        
        .instructor-name {
            font-size: 1.4rem;
            font-weight: 600;
            color: var(--dark-text);
            margin-bottom: 0.5rem;
        }
        
        .instructor-title {
            font-size: 1rem;
            font-weight: 500;
            color: var(--nvidia-green);
            margin-bottom: 1rem;
        }
        
        .instructor-bio {
            color: var(--light-text);
            line-height: 1.6;
            text-align: left;
        }
        
        .outcomes-list {
            list-style: none;
            padding: 0;
        }
        
        .outcomes-list li {
            padding: 0.75rem 0;
            border-bottom: 1px solid #e9ecef;
            display: flex;
            align-items: flex-start;
            gap: 0.75rem;
        }
        
        .outcomes-list li:last-child {
            border-bottom: none;
        }
        
        .outcomes-list li::before {
            content: "✅";
            font-size: 1.2rem;
            flex-shrink: 0;
            margin-top: 0.1rem;
        }
        
        .requirement-section {
            margin: 1.5rem 0;
        }
        
        .requirement-section h3 {
            color: var(--dark-text);
            font-weight: 600;
            margin-bottom: 1rem;
            font-size: 1.2rem;
        }
        
        .requirement-section ul {
            list-style-type: none;
            padding: 0;
        }
        
        .requirement-section ul li {
            padding: 0.5rem 0;
            border-bottom: 1px solid #f0f0f0;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .requirement-section ul li:last-child {
            border-bottom: none;
        }
        
        .requirement-section ul li::before {
            content: "🔧";
            font-size: 1rem;
        }
        
        @media (max-width: 768px) {
            .page-header {
                padding: 2rem 0;
            }
            
            .main-content {
                padding: 1.5rem 1rem;
                margin-top: -1rem;
            }
            
            .event-detail {
                flex-direction: column;
                gap: 0.2rem;
            }
            
            .instructor-grid {
                grid-template-columns: 1fr;
            }
            
            .item-header {
                flex-direction: column;
                align-items: flex-start;
            }
            
            .item-duration {
                align-self: flex-start;
            }
        }
    </style>
</head>
<body>
    <header class="page-header">
        <h1 class="project-name">🧠 Incentivizing Reasoning in LLMs</h1>
        <p class="project-tagline">
            KDD 2025 Tutorial: Practical Guidance on Building Reasoning Capabilities using Distillation and Reinforcement Learning
        </p>
    </header>

    <div class="main-content">
        <div class="brand-badges">
            <span class="brand-badge nvidia-badge">🚀 NVIDIA Powered</span>
            <span class="brand-badge kdd-badge">🏆 KDD 2025 Official</span>
        </div>

        <div class="event-info">
            <div class="event-detail">📅 August 3-7, 2025</div>
            <div class="event-detail">📍 Toronto, Canada | KDD 2025 Conference</div>
            <div class="event-detail">🤝 Presented by <strong>NVIDIA</strong> Deep Learning Solution Architects</div>
        </div>

        <h2>🎯 Tutorial Abstract</h2>
        <div class="modern-card">
            <p>With reasoning models like <strong>DeepSeek-R1</strong> and <strong>OpenAI's o1</strong> demonstrating breakthrough capabilities in complex problem-solving, there's growing interest in the AI community about how to unlock similar capabilities in other large language models (LLMs).</p>
            
            <p>This hands-on tutorial dives into practical methods for building reasoning capabilities in LLMs through two primary approaches:</p>
            <ul>
                <li><strong>Knowledge Distillation</strong>: Transferring capabilities from advanced reasoning models</li>
                <li><strong>Reinforcement Learning</strong>: Further enhancing capabilities through post-training techniques</li>
            </ul>
            
            <p>Participants will learn how to transfer reasoning capabilities from cutting-edge models like DeepSeek-R1 into smaller LLMs such as Qwen and Llama, and then explore how reinforcement learning can take these capabilities even further.</p>
        </div>

        <h2>👥 Target Audience and Prerequisites</h2>
        <div class="modern-card">
            <p><strong>Target audience</strong>: Data scientists and engineers who are interested in enhancing reasoning capabilities in LLMs for downstream tasks.</p>
            
            <p><strong>Skill Level</strong>: The tutorial will be designed to accommodate participants with varying levels of expertise, from beginners to moderately skilled users, with the pace set to ensure beginners can follow comfortably.</p>
            
            <p><strong>Prerequisites</strong>:</p>
            <ul>
                <li>Basic knowledge of deep learning and LLM concepts</li>
                <li>Familiarity with Python programming</li>
            </ul>
        </div>

        <h2>📚 Tutorial Outline</h2>
        <div class="outline-grid">
            <div class="outline-item">
                <div class="item-header">
                    <div class="item-title">🌟 Introduction and Lab Overview</div>
                    <div class="item-duration">⏱️ 20 Minutes</div>
                </div>
                <div class="item-description">
                    This part covers the core methodologies that can be leveraged to incentivize reasoning capabilities into large language models (LLMs). First, we will explain how knowledge distillation works for transferring reasoning abilities from large models to smaller ones, covering concepts like long Chain-of-Thought data and teacher-student frameworks. Then, we will introduce how reinforcement learning (RL) can be applied to post-train LLMs for enhanced reasoning, including reward design and various RL algorithms.
                </div>
            </div>

            <div class="outline-item">
                <div class="item-header">
                    <div class="item-title">🔧 Hands-on: Setting Up the Environment for the Lab</div>
                    <div class="item-duration">⏱️ 10 Minutes</div>
                </div>
                <div class="item-description">
                    In this part we will introduce the lab environment, the libraries, frameworks, and datasets used in the exercise. We'll guide the participants to conduct a quick verification process to ensure everyone's environment is correctly configured.
                </div>
            </div>

            <div class="outline-item">
                <div class="item-header">
                    <div class="item-title">🔍 Hands-on: Extracting Reasoning Data</div>
                    <div class="item-duration">⏱️ 30 Minutes</div>
                </div>
                <div class="item-description">
                    In this lab, participants will learn how to automatically generate long Chain-of-Thought data that encapsulates reasoning processes from advanced reasoning models, such as DeepSeek-R1. We'll demonstrate how to filter low-quality data and prepare it for distillation into smaller models using NeMo's data processing tools.
                </div>
            </div>

            <div class="outline-item">
                <div class="item-header">
                    <div class="item-title">🧪 Hands-on: Distilling Reasoning Capability into Smaller Models</div>
                    <div class="item-duration">⏱️ 60 Minutes</div>
                </div>
                <div class="item-description">
                    This lab will walk participants through implementing knowledge distillation in open-source models like Qwen and Llama using NVIDIA's NeMo framework. We'll cover the technical details of setting up distillation training experiment in NeMo, monitoring training progress effectively, and evaluating the results.
                </div>
            </div>

            <div class="outline-item">
                <div class="item-header">
                    <div class="item-title">🚀 Hands-on: Post-training using Reinforcement Learning</div>
                    <div class="item-duration">⏱️ 60 Minutes</div>
                </div>
                <div class="item-description">
                    This lab will teach participants how to set up a reinforcement learning environment for post-training LLMs to further enhance reasoning capabilities. We'll cover topics including how to use the RL frameworks, how to design reward functions, and guide participants through the whole fine-tuning process using RL.
                </div>
            </div>

            <div class="outline-item">
                <div class="item-header">
                    <div class="item-title">🎓 Conclusion and Resources</div>
                    <div class="item-duration">⏱️ 20 Minutes</div>
                </div>
                <div class="item-description">
                    In conclusion, this tutorial will revisit the two post-training approaches discussed earlier, summarizing their suitable applications, limitations, and unresolved challenges. Participants will also receive a comprehensive set of resources, including online Jupyter Notebook tutorials, curated "awesome lists," and best practices for distillation and reinforcement learning (RL) training.
                </div>
            </div>
        </div>

        <h2>👨‍🏫 Meet Our Instructors</h2>
        <div class="instructor-grid">
            <div class="instructor-card">
                <div class="instructor-name">Zhaopeng Qiu</div>
                <div class="instructor-title">NVIDIA Solution Architect</div>
                <div class="instructor-bio">
                    Currently working at NVIDIA as a Deep Learning Solution Architect. He graduated from Peking University in 2018. His research focuses on large language models (LLMs), recommender systems, and natural language processing (NLP). He has authored over 20 research papers published in leading journals and conferences, including KDD, AAAI, WWW, TKDE, NAACL, COLING, and others.
                </div>
            </div>

            <div class="instructor-card">
                <div class="instructor-name">Jingqi Zhang</div>
                <div class="instructor-title">NVIDIA Solution Architect</div>
                <div class="instructor-bio">
                    Currently a Solution Architect at NVIDIA, specializing in large language models (LLMs). His work focuses on various aspects of LLMs including training methodologies, practical applications, and reasoning models. Prior to joining NVIDIA, Jingqi obtained both his Bachelor's and Master's degrees in Computer Science and Technology from Xi'an Jiaotong University.
                </div>
            </div>

            <div class="instructor-card">
                <div class="instructor-name">Shuang Yu</div>
                <div class="instructor-title">NVIDIA Solution Architect</div>
                <div class="instructor-bio">
                    A Solution Architect at NVIDIA focusing on LLMs. She holds a Bachelor's Degree in Automation and a Master's Degree in Computer Science from Tsinghua University. Before joining NVIDIA, Shuang worked as a software architect at IBM, where she led the development of an enterprise-level machine learning platform.
                </div>
            </div>
        </div>

        <h2>🛠️ Technical Requirements</h2>
        <div class="modern-card">
            <div class="requirement-section">
                <h3>Technical Resources</h3>
                <ul>
                    <li>Jupyter notebooks</li>
                    <li>APIs for accessing various reasoning models (free tiers)</li>
                    <li>NeMo framework (open-source)</li>
                    <li>Sample datasets for distillation and RL training</li>
                    <li>Open-source models (Qwen, Llama)</li>
                </ul>
            </div>

            <div class="requirement-section">
                <h3>Software Environment</h3>
                <ul>
                    <li>Python 3.8+</li>
                    <li>CUDA-compatible GPU (recommended)</li>
                    <li>Docker (optional)</li>
                </ul>
            </div>
        </div>

        <h2>🎯 Learning Outcomes</h2>
        <div class="modern-card">
            <p>By the end of this session, participants will be equipped with:</p>
            <ul class="outcomes-list">
                <li><strong>Theoretical Understanding</strong>: Core concepts of knowledge distillation and reinforcement learning for reasoning</li>
                <li><strong>Practical Skills</strong>: Hands-on experience in data preparation and processing techniques</li>
                <li><strong>Technical Implementation</strong>: Ability to use NeMo framework for model distillation experiments</li>
                <li><strong>Advanced Techniques</strong>: Knowledge of applying RL for post-training reasoning enhancement</li>
                <li><strong>Real-world Application</strong>: Practical experience that can be applied to their own projects</li>
            </ul>
        </div>

        <h2>🔗 Important Links</h2>
        <div class="modern-card">
            <ul>
                <li>🌐 <strong>Tutorial Website</strong>: <a href="https://zpqiu.github.io/reasoning-model-tutorial-kdd2025/">zpqiu.github.io/reasoning-model-tutorial-kdd2025</a></li>
                <li>📚 <strong>KDD 2025 Official</strong>: <a href="https://kdd2025.kdd.org/">kdd2025.kdd.org</a></li>
                <li>🛠️ <strong>NeMo Framework</strong>: <a href="https://nvidia.github.io/NeMo/">nvidia.github.io/NeMo</a></li>
                <li>📖 <strong>Key Papers</strong>: DeepSeek R1, Knowledge Distillation in LLMs, RL for LLM Training</li>
            </ul>
        </div>
    </div>

    <div class="footer-info">
        <p>🤝 This tutorial is carefully prepared by <strong class="nvidia-text">NVIDIA</strong> Deep Learning Solution Architects</p>
        <p>🎓 Presented as an official <strong class="kdd-text">KDD 2025</strong> Tutorial</p>
        <p>💡 Dedicated to advancing AI reasoning technology and applications</p>
    </div>
</body>
</html> 